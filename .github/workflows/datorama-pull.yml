name: Monthly Investment Analysis Update

on:
  schedule:
    # Run on the 1st day of every month at 6 AM UTC
    - cron: '0 6 1 * *'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      force_scrape:
        description: 'Force fresh data scraping (ignores cache)'
        required: false
        default: 'false'
        type: boolean

jobs:
  monthly-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Generous timeout for complete pipeline
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Playwright and browsers
      run: |
        # Install Playwright with optimizations for headless Ubuntu
        playwright install chromium
        playwright install-deps chromium

    - name: Configure environment for headless operation
      run: |
        # Set up headless display for GUI applications
        export DISPLAY=:99
        export PLAYWRIGHT_BROWSERS_PATH=$HOME/.cache/ms-playwright
        
        # Configure timeouts for stable headless operation
        echo "PLAYWRIGHT_TIMEOUT=60000" >> $GITHUB_ENV
        echo "SCRAPER_DELAY=2" >> $GITHUB_ENV
        echo "MAX_RETRIES=3" >> $GITHUB_ENV

    - name: Check existing cache age
      id: cache-check
      run: |
        if [ -f "cache/holdings.json" ] && [ -f "cache/history.json" ]; then
          # Get cache file modification time
          cache_age=$(stat -c %Y cache/holdings.json)
          current_time=$(date +%s)
          age_days=$(( (current_time - cache_age) / 86400 ))
          
          echo "Cache age: $age_days days"
          echo "cache_age_days=$age_days" >> $GITHUB_OUTPUT
          
          if [ $age_days -gt 25 ] || [ "${{ github.event.inputs.force_scrape }}" = "true" ]; then
            echo "Cache is stale or force scrape requested - will refresh"
            echo "should_scrape=true" >> $GITHUB_OUTPUT
          else
            echo "Cache is fresh - will use existing data"
            echo "should_scrape=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "No cache found - will scrape fresh data"
          echo "should_scrape=true" >> $GITHUB_OUTPUT
          echo "cache_age_days=999" >> $GITHUB_OUTPUT
        fi

    - name: Scrape fresh investment data
      if: steps.cache-check.outputs.should_scrape == 'true'
      timeout-minutes: 45
      run: |
        echo "🔄 Starting fresh data scraping..."
        echo "Environment: Headless Ubuntu with optimized timeouts"
        
        # Run scraper with headless-optimized settings
        python3 datorama_scrape.py \
          --headless \
          --timeout 60 \
          --delay 2 \
          --max-retries 3 \
          --user-agent "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        
        echo "✅ Data scraping completed successfully"
        
        # Verify scraped data quality
        echo "📊 Data validation:"
        echo "Holdings: $(cat cache/holdings.json | jq '. | length') records"
        echo "History: $(cat cache/history.json | jq '. | length') activities"

    - name: Generate comprehensive investment analysis
      timeout-minutes: 15
      run: |
        echo "🔬 Starting comprehensive investment analysis..."
        
        # Remove old analysis to ensure fresh generation
        rm -rf analysis/
        
        # Generate complete analysis suite
        python3 analyze_holdings.py
        
        echo "✅ Analysis generation completed successfully"

    - name: Validate analysis outputs
      run: |
        echo "🔍 Validating analysis outputs..."
        
        # Check main analysis directory
        if [ ! -d "analysis" ]; then
          echo "❌ Analysis directory not created"
          exit 1
        fi
        
        # Count and validate main analysis files
        csv_count=$(ls analysis/*.csv 2>/dev/null | wc -l)
        visual_count=$(ls analysis/visuals/*.png 2>/dev/null | wc -l)
        
        echo "📊 Main analysis: $csv_count CSV files, $visual_count visualizations"
        
        if [ $csv_count -lt 20 ]; then
          echo "❌ Insufficient main analysis files (expected 20+, got $csv_count)"
          exit 1
        fi
        
        if [ $visual_count -lt 3 ]; then
          echo "❌ Insufficient main visualizations (expected 3+, got $visual_count)"
          exit 1
        fi
        
        # Check historical analysis
        hist_csv_count=$(ls analysis/historical/*.csv 2>/dev/null | wc -l)
        hist_visual_count=$(ls analysis/historical/visuals/*.png 2>/dev/null | wc -l)
        
        echo "📈 Historical analysis: $hist_csv_count CSV files, $hist_visual_count visualizations"
        
        if [ $hist_csv_count -lt 5 ]; then
          echo "❌ Insufficient historical analysis files (expected 5+, got $hist_csv_count)"
          exit 1
        fi
        
        if [ $hist_visual_count -lt 3 ]; then
          echo "❌ Insufficient historical visualizations (expected 3+, got $hist_visual_count)"
          exit 1
        fi
        
        # Verify critical files exist
        critical_files=(
          "analysis/interesting_stocks_overview.csv"
          "analysis/manager_performance.csv"
          "analysis/high_conviction_stocks.csv"
          "analysis/historical/manager_performance_3year_summary.csv"
          "analysis/historical/quarterly_activity_timeline.csv"
          "analysis/visuals/comprehensive_holdings_overview.png"
          "analysis/historical/visuals/manager_performance_3year.png"
          "README.md"
        )
        
        for file in "${critical_files[@]}"; do
          if [ ! -f "$file" ]; then
            echo "❌ Critical file missing: $file"
            exit 1
          fi
        done
        
        echo "✅ All analysis outputs validated successfully"

    - name: Generate performance summary
      id: summary
      run: |
        echo "📊 Generating performance summary..."
        
        # Extract key metrics for summary
        total_holdings=$(cat cache/holdings.json | jq '. | length')
        total_activities=$(cat cache/history.json | jq '. | length')
        manager_count=$(cat cache/holdings.json | jq '[.[].manager] | unique | length')
        
        # Get top stocks from analysis
        top_stock=$(head -2 analysis/interesting_stocks_overview.csv | tail -1 | cut -d',' -f1)
        top_conviction=$(head -2 analysis/high_conviction_stocks.csv | tail -1 | cut -d',' -f1)
        
        # Calculate data freshness
        cache_date=$(stat -c %Y cache/holdings.json)
        cache_human=$(date -d @$cache_date '+%Y-%m-%d %H:%M UTC')
        
        echo "summary_holdings=$total_holdings" >> $GITHUB_OUTPUT
        echo "summary_activities=$total_activities" >> $GITHUB_OUTPUT
        echo "summary_managers=$manager_count" >> $GITHUB_OUTPUT
        echo "summary_top_stock=$top_stock" >> $GITHUB_OUTPUT
        echo "summary_top_conviction=$top_conviction" >> $GITHUB_OUTPUT
        echo "summary_cache_date=$cache_human" >> $GITHUB_OUTPUT

    - name: Check for changes and commit
      id: git-check
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action - Monthly Analysis"
        
        # Check if there are changes to commit
        git add .
        
        if git diff --cached --quiet; then
          echo "No changes to commit"
          echo "has_changes=false" >> $GITHUB_OUTPUT
        else
          echo "Changes detected - preparing commit"
          echo "has_changes=true" >> $GITHUB_OUTPUT
          
          # Show what's being committed
          echo "📋 Files being updated:"
          git diff --cached --name-only | head -20
          if [ $(git diff --cached --name-only | wc -l) -gt 20 ]; then
            echo "... and $(( $(git diff --cached --name-only | wc -l) - 20 )) more files"
          fi
        fi

    - name: Commit and push updates
      if: steps.git-check.outputs.has_changes == 'true'
      run: |
        # Create comprehensive commit message
        git commit -m "🤖 Monthly Investment Analysis Update - $(date '+%B %Y')
        
        # Push changes
        git push origin main

    - name: Create workflow summary
      if: always()
      run: |
        echo "## 🤖 Monthly Investment Analysis Pipeline" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Execution Summary:**" >> $GITHUB_STEP_SUMMARY
        echo "- **Date:** $(date '+%B %d, %Y at %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
        echo "- **Data Scraping:** ${{ steps.cache-check.outputs.should_scrape == 'true' && '✅ Fresh data collected' || '📋 Used existing cache' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Cache Age:** ${{ steps.cache-check.outputs.cache_age_days }} days" >> $GITHUB_STEP_SUMMARY
        echo "- **Analysis Generated:** ${{ steps.git-check.outputs.has_changes == 'true' && '✅ Complete analysis updated' || '📋 No changes detected' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Market Data Processed:**" >> $GITHUB_STEP_SUMMARY
        echo "- **Current Holdings:** ${{ steps.summary.outputs.summary_holdings }} positions" >> $GITHUB_STEP_SUMMARY
        echo "- **Historical Activities:** ${{ steps.summary.outputs.summary_activities }} transactions" >> $GITHUB_STEP_SUMMARY
        echo "- **Institutional Managers:** ${{ steps.summary.outputs.summary_managers }} tracked" >> $GITHUB_STEP_SUMMARY
        echo "- **Data Coverage:** 18+ years (2007-2025)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Key Investment Insights:**" >> $GITHUB_STEP_SUMMARY
        echo "- **Top Stock Opportunity:** ${{ steps.summary.outputs.summary_top_stock }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Highest Conviction Play:** ${{ steps.summary.outputs.summary_top_conviction }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Analysis Reports:** 28+ comprehensive CSV files" >> $GITHUB_STEP_SUMMARY
        echo "- **Visualizations:** 9 enhanced charts and graphs" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Next Scheduled Update:** $(date -d 'next month' '+%B 1, %Y')" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📊 **View Analysis:** [README.md](../README.md) | 📁 **Browse Data:** [analysis/](../analysis/)" >> $GITHUB_STEP_SUMMARY

    - name: Handle failures
      if: failure()
      run: |
        echo "❌ Monthly analysis pipeline failed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Failure Details:**" >> $GITHUB_STEP_SUMMARY
        echo "- **Job:** ${{ github.job }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Step:** Check workflow logs for specific failure point" >> $GITHUB_STEP_SUMMARY
        echo "- **Cache Age:** ${{ steps.cache-check.outputs.cache_age_days }} days" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Troubleshooting:**" >> $GITHUB_STEP_SUMMARY
        echo "1. Check if Playwright installation succeeded" >> $GITHUB_STEP_SUMMARY
        echo "2. Verify network connectivity for data scraping" >> $GITHUB_STEP_SUMMARY
        echo "3. Review analysis code for any syntax errors" >> $GITHUB_STEP_SUMMARY
        echo "4. Consider running workflow_dispatch manually for debugging" >> $GITHUB_STEP_SUMMARY
        
        # Create issue comment if running on schedule
        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "Scheduled monthly analysis failed - manual intervention may be required"
        fi