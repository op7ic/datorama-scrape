name: Monthly Investment Analysis Update

on:
  schedule:
    # Run on the 1st day of every month at 6 AM UTC
    - cron: '0 6 1 * *'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      force_scrape:
        description: 'Force fresh data scraping (ignores cache)'
        required: false
        default: 'false'
        type: boolean

jobs:
  investment-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Playwright
      run: |
        playwright install chromium
        playwright install-deps chromium

    - name: Create cache directory
      run: |
        mkdir -p cache
        mkdir -p analysis
        mkdir -p log

    - name: Run enhanced data scraper
      id: scrape
      run: |
        echo "ðŸš€ Starting enhanced Dataroma scraper with Yahoo Finance integration..."
        
        # Run the scraper with timeout
        if timeout 2100 python3 datorama_scrape.py; then
          echo "âœ… Enhanced scraper completed successfully"
          echo "status=success" >> $GITHUB_OUTPUT
        else
          echo "âš ï¸ Enhanced scraper failed or timed out"
          echo "status=failed" >> $GITHUB_OUTPUT
        fi
        
        # Check if we got valid data
        if [ -f "cache/holdings.json" ] && [ -f "cache/history.json" ]; then
          holdings_count=$(cat cache/holdings.json | jq '. | length' 2>/dev/null || echo "0")
          history_count=$(cat cache/history.json | jq '. | length' 2>/dev/null || echo "0")
          
          if [ "$holdings_count" -gt "100" ] && [ "$history_count" -gt "1000" ]; then
            echo "âœ… Valid data scraped: $holdings_count holdings, $history_count activities"
            echo "data_valid=true" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ Insufficient data: $holdings_count holdings, $history_count activities"
            echo "data_valid=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "âŒ Cache files not created"
          echo "data_valid=false" >> $GITHUB_OUTPUT
        fi

    - name: Fallback - Use cached data
      if: steps.scrape.outputs.data_valid != 'true'
      run: |
        echo "ðŸ“¥ Using fallback mode - checking for existing data..."
        
        # Check if we have any previous data
        if [ -f "cache/holdings.json" ] && [ -f "cache/history.json" ]; then
          echo "âœ… Previous cache data found - will proceed with analysis"
        else
          echo "âš ï¸ No previous data available - creating minimal dataset"
          cat > cache/holdings.json << 'EOF'
        []
        EOF
          cat > cache/history.json << 'EOF'
        []
        EOF
        fi

    - name: Generate comprehensive analysis
      if: always()
      run: |
        echo "ðŸ“Š Starting comprehensive investment analysis..."
        python3 analyze_holdings.py || echo "âš ï¸ Analysis completed with warnings"
        echo "âœ… Analysis generation completed"

    - name: Verify analysis outputs
      run: |
        echo "ðŸ” Verifying analysis outputs..."
        
        # Check main analysis files
        if [ -d "analysis/" ]; then
          echo "Found $(find analysis/ -name "*.csv" | wc -l) analysis CSV files"
          echo "Found $(find analysis/ -name "*.png" | wc -l) visualization files"
        fi
        
        # Verify key files exist (if they should)
        if [ -f "analysis/interesting_stocks_overview.csv" ]; then
          echo "âœ… Main analysis files verified"
        else
          echo "âš ï¸ Some analysis files may be missing"
        fi

    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        git diff --staged --quiet || git commit -m "Update $(date -u +'%Y-%m-%d')"
        git push || echo "No changes to push"
